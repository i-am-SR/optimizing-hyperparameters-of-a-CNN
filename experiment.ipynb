{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHydg_10HsGt",
        "colab_type": "code",
        "outputId": "90c4386d-004f-4402-aa2e-1ccd0ab6828e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec  7 20:02:11 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.33.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMDrFvIOaA1N",
        "colab_type": "code",
        "outputId": "36100b60-ae20-4d6f-8c97-1bbc6c66db6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(42)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wwpntgt1K9Df",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuoQi7kRX1-Z",
        "colab_type": "text"
      },
      "source": [
        "# Model & Data Setup\n",
        "You should not have to make any modifications to the below code (unless your\n",
        "specific part requires it)\n",
        "\n",
        "Also -- the changing Learning Rate has not been implemented in the model, so \n",
        "that will require a change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HywthfQaLlWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(loc):\n",
        "    data = loadmat(loc)\n",
        "    return data['X'], data['y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGpPZzwOLpor",
        "colab_type": "code",
        "outputId": "346e8c69-8af8-443c-d7db-4eaeab731afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "if not os.path.exists('data'):\n",
        "    ! mkdir data\n",
        "    ! wget http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
        "    ! wget http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
        "    # ! wget http://ufldl.stanford.edu/housenumbers/extra_32x32.mat\n",
        "    ! mv *.mat data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-07 20:02:18--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182040794 (174M) [text/plain]\n",
            "Saving to: ‘train_32x32.mat’\n",
            "\n",
            "train_32x32.mat     100%[===================>] 173.61M  14.8MB/s    in 14s     \n",
            "\n",
            "2019-12-07 20:02:33 (12.0 MB/s) - ‘train_32x32.mat’ saved [182040794/182040794]\n",
            "\n",
            "--2019-12-07 20:02:34--  http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64275384 (61M) [text/plain]\n",
            "Saving to: ‘test_32x32.mat’\n",
            "\n",
            "test_32x32.mat      100%[===================>]  61.30M  13.9MB/s    in 5.7s    \n",
            "\n",
            "2019-12-07 20:02:40 (10.7 MB/s) - ‘test_32x32.mat’ saved [64275384/64275384]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUjGSSctLlWm",
        "colab_type": "code",
        "outputId": "d10f4449-2d47-4c90-c2b6-03d2f10f92f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "X_train, Y_train = load_data('data/train_32x32.mat')\n",
        "X_test, Y_test = load_data('data/test_32x32.mat')\n",
        "X_train = np.transpose(X_train, (3,0,1,2))\n",
        "X_test = np.transpose(X_test, (3,0,1,2))\n",
        "print(\"Training Set\", X_train.shape, Y_train.shape)\n",
        "print(\"Test Set\", X_test.shape, Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set (73257, 32, 32, 3) (73257, 1)\n",
            "Test Set (26032, 32, 32, 3) (26032, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtNwdGLFLlWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalizer = 1/255\n",
        "X_train = X_train * normalizer\n",
        "X_test = X_test * normalizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F8BkS4oLlXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_size(trX, trY, tsX, tsY, trNum=10000, tsNum=2000, valNum=500):\n",
        "    vals = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    newTrX = []\n",
        "    newTrY = []\n",
        "    newTsX = []\n",
        "    newTsY = []\n",
        "    valX = []\n",
        "    valY = []\n",
        "    for i in vals:\n",
        "        indices = np.where(trY == [i])[0]\n",
        "        for num in range((int)(trNum/10)):\n",
        "            newTrX.append(trX[indices[num]])\n",
        "            newTrY.append(trY[indices[num]][0])\n",
        "        indices = np.where(tsY == [i])[0]\n",
        "        for num in range(((int)(tsNum/10))+((int)(valNum/10))):\n",
        "            if num < (tsNum/10):\n",
        "                newTsX.append(tsX[indices[num]])\n",
        "                newTsY.append(tsY[indices[num]][0])\n",
        "            else:\n",
        "                #valX.append(tsX[indices[num]])\n",
        "                valX.append(np.copy(tsX[indices[num]]))\n",
        "                valY.append(tsY[indices[num]][0])\n",
        "    \n",
        "    return np.asarray(newTrX).reshape(trNum, 32, 32, 3), np.asarray(newTrY).reshape(trNum, 1), np.asarray(newTsX).reshape(tsNum, 32, 32, 3), np.asarray(newTsY).reshape(tsNum, 1), np.asarray(valX).reshape(valNum, 32, 32, 3), np.asarray(valY).reshape(valNum,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVIHEUiBLlXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trX, trY, tsX, tsY, valX, valY = data_size(X_train, Y_train, X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJk5SscfLlXQ",
        "colab_type": "code",
        "outputId": "e6e85b85-5b01-4162-a2a2-e16b1567b2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "print(trX.shape)\n",
        "print(trY.shape)\n",
        "print(tsX.shape)\n",
        "print(tsY.shape)\n",
        "print(valX.shape)\n",
        "print(valY.shape)\n",
        "trY[trY == 10] = 0\n",
        "tsY[tsY == 10] = 0\n",
        "valY[valY == 10] = 0\n",
        "trY = keras.utils.to_categorical(trY, 10)\n",
        "tsY = keras.utils.to_categorical(tsY, 10)\n",
        "valY = keras.utils.to_categorical(valY, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 32, 32, 3)\n",
            "(10000, 1)\n",
            "(2000, 32, 32, 3)\n",
            "(2000, 1)\n",
            "(500, 32, 32, 3)\n",
            "(500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM6NjYKkMEVS",
        "colab_type": "code",
        "outputId": "b2d14e37-f73a-4769-bb47-69131935a35c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "print(trX.shape)\n",
        "print(trY.shape)\n",
        "print(tsX.shape)\n",
        "print(tsY.shape)\n",
        "print(valX.shape)\n",
        "print(valY.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 32, 32, 3)\n",
            "(10000, 10)\n",
            "(2000, 32, 32, 3)\n",
            "(2000, 10)\n",
            "(500, 32, 32, 3)\n",
            "(500, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6-34B9WeLmcj",
        "colab": {}
      },
      "source": [
        "# DO NOT MODIFY THIS METHOD\n",
        "class CNN_Model():\n",
        "    def __init__(self, lr_0=0.01, delta=1e-6, B=32, p_1=0.1, p_2=0.1):\n",
        "        '''\n",
        "        All the parameters exposed are those we need to tune for the]\n",
        "        project\n",
        "        '''\n",
        "\n",
        "        self.lr_0 = lr_0 # Starting Learning Rate\n",
        "        self.delta = delta # Learning Rate Decay\n",
        "        self.B = B # Mini-Batch Size\n",
        "        self.p_1 = p_1 # Dropout Parameter 1\n",
        "        self.p_2 = p_2 # Dropout Parameter 2\n",
        "    \n",
        "    def define_model(self):\n",
        "        '''\n",
        "        Defines the model with the given parameters\n",
        "        TODO: Learning Rate Decay in Keras\n",
        "        '''\n",
        "        DATAFORMAT = \"channels_last\"\n",
        "\n",
        "        # Input to model\n",
        "        inputs = Input(shape=(32, 32, 3), name='input') # TODO: assuming input size is 784\n",
        "        # Convolutional Layers\n",
        "        conv1 = Conv2D(filters=32, kernel_size=(5,5), padding='same',\n",
        "                       data_format=DATAFORMAT, strides=1, name='conv1')(inputs)\n",
        "        conv2 = Conv2D(filters=64, kernel_size=(3,3), padding='same',\n",
        "                       data_format=DATAFORMAT, strides=1, name='conv2')(conv1)\n",
        "        conv3 = Conv2D(filters=128, kernel_size=(3,3), padding='same',\n",
        "                       data_format=DATAFORMAT, strides=1, name='conv3')(conv2)\n",
        "\n",
        "        flatten = Flatten(data_format=DATAFORMAT)(conv3)\n",
        "        \n",
        "        # Fully-Connected Layers\n",
        "        fc1 = Dense(1024, activation='relu', name='fc1')(flatten)\n",
        "        drop1 = Dropout(self.p_1, name='drop1')(fc1)\n",
        "        fc2 = Dense(1024, activation='relu', name='fc2')(drop1)\n",
        "        drop2 = Dropout(self.p_2, name='drop2')(fc2)\n",
        "        fc3 = Dense(10, activation='softmax', name='output')(drop2)\n",
        "\n",
        "        model_ = Model(inputs=inputs, outputs=fc3)\n",
        "        optim = SGD(lr=self.lr_0, decay=self.delta)\n",
        "        model_.compile(optimizer=optim,\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "        self.model = model_\n",
        "    \n",
        "    def train_model(self, x, y, valX=None, valY=None, epochs=15):\n",
        "        '''\n",
        "        Trains the model.\n",
        "        '''\n",
        "        val_data = (valX, valY)\n",
        "        self.train_history = self.model.fit(x, y, batch_size=self.B, epochs=epochs, verbose=0,\n",
        "                    validation_data=val_data)\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ODgEk51vPWm0",
        "outputId": "4be8089a-ebb6-4d2d-9272-4bf4e0a5e10f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "source": [
        "cnn = CNN_Model(B=1024)\n",
        "cnn.define_model()\n",
        "plot_model(cnn.model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKgAAAOoCAIAAADKwSh+AAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ldr7WG1wR6d1",
        "colab": {}
      },
      "source": [
        "#cnn.train_model(trX, trY, valX, valY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx1cgMJIXyWM",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameter Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3xOipVxw2Eb",
        "colab_type": "text"
      },
      "source": [
        "## Using Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGg-JReMtfRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_lr = [1e-1, 1e-3, 1e-5, 1e-6] # ref: https://bit.ly/2L9Frj7\n",
        "grid_delta = [0, 1e-1, 1e-3, 1e-4, 1e-6, 1e-8, 1e-10]\n",
        "grid_B = [32, 64, 128, 256, 512, 1024][::-1]\n",
        "grid_p1 = [0.1*i for i in range(1,6)]\n",
        "grid_p2 = [0.1*i for i in range(1,6)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huIHpN6ZvWwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_lr = [1e-3, 1e-5, 1e-6] # ref: https://bit.ly/2L9Frj7\n",
        "grid_delta = [0, 1e-1, 1e-3, 1e-4, 1e-6, 1e-8, 1e-10]\n",
        "grid_B = [32, 64, 128, 256, 512, 1024][::-1]\n",
        "grid_p1 = [0.1*i for i in range(1,6)]\n",
        "grid_p2 = [0.1*i for i in range(1,6)]\n",
        "\n",
        "from random import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ6ZVIejvEDj",
        "colab_type": "code",
        "outputId": "0455eaa2-eba2-405a-cd4d-19f2cd5197f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        }
      },
      "source": [
        "# Grid Search!\n",
        "best_val_loss = 1.4572559852600098\n",
        "best_params = {'lr': 0.1, 'delta': 0.1, 'B': 128, 'p1': 0.4, 'p2': 0.1, 'iteration': 241}\n",
        "gs_iteration = 885\n",
        "best_iteration = 241\n",
        "search_space = []\n",
        "## hacky but gets the job done\n",
        "for lr in grid_lr:\n",
        "    for delta in grid_delta:\n",
        "        for B in grid_B:\n",
        "            for p1 in grid_p1:\n",
        "                for p2 in grid_p2:\n",
        "                    search_space.append((lr, delta, B, p1, p2))\n",
        "shuffle(search_space)\n",
        "for lr, delta, B, p1, p2 in search_space:\n",
        "    gs_iteration += 1\n",
        "    if (gs_iteration % 5) == 0:\n",
        "        print(f'Iteration {gs_iteration}: Val Loss={best_val_loss}. Params={best_params}')\n",
        "    cnn = CNN_Model(lr_0=lr, delta=delta, B=B, p_1=p1, p_2=p2)\n",
        "    cnn.define_model()\n",
        "    cnn.train_model(trX, trY, valX, valY)\n",
        "    val_loss = np.nanmin(cnn.train_history.history['val_loss']) # ignore nan's\n",
        "    del(cnn)\n",
        "    K.clear_session()\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_params = {'lr':lr, 'delta':delta, 'B':B, 'p1': p1,\n",
        "                        'p2': p2, 'iteration': gs_iteration}\n",
        "        best_iteration = gs_iteration\n",
        "        print('New best val loss!')\n",
        "        print(f'Iteration {gs_iteration}: Val Loss={best_val_loss}. Params={best_params}')                    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 890: Val Loss=1.4572559852600098. Params={'lr': 0.1, 'delta': 0.1, 'B': 128, 'p1': 0.4, 'p2': 0.1, 'iteration': 241}\n",
            "Iteration 895: Val Loss=1.4572559852600098. Params={'lr': 0.1, 'delta': 0.1, 'B': 128, 'p1': 0.4, 'p2': 0.1, 'iteration': 241}\n",
            "Iteration 900: Val Loss=1.4572559852600098. Params={'lr': 0.1, 'delta': 0.1, 'B': 128, 'p1': 0.4, 'p2': 0.1, 'iteration': 241}\n",
            "New best val loss!\n",
            "Iteration 900: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 905: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 910: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 915: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 920: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 925: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 930: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 935: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 940: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 945: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 950: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 955: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 960: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 965: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 970: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 975: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 980: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 985: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 990: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 995: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 1000: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 1005: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 1010: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 1015: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "Iteration 1020: Val Loss=1.3852515563964845. Params={'lr': 0.001, 'delta': 0, 'B': 32, 'p1': 0.2, 'p2': 0.1, 'iteration': 900}\n",
            "New best val loss!\n",
            "Iteration 1020: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1025: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1030: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1035: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1040: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1045: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1050: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1055: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1060: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1065: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1070: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1075: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1080: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1085: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1090: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1095: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n",
            "Iteration 1100: Val Loss=1.3616904706954955. Params={'lr': 0.001, 'delta': 1e-08, 'B': 32, 'p1': 0.2, 'p2': 0.2, 'iteration': 1020}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdQRycLW3nKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Best Validation Loss:\\t {best_val_loss}')\n",
        "print(f'Best parameters:\\t {best_params} ')\n",
        "print(f'Number of total iterations: \\t {gs_iteration}')\n",
        "print(f'Best iteration:\\t {best_iteration}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1BeBygHw50S",
        "colab_type": "text"
      },
      "source": [
        "## Using Bayesian Global Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VECaYEzqw7-s",
        "colab_type": "code",
        "outputId": "180f77e5-9844-4a3d-a8a9-4e5459ffa0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "!pip install bayesian-optimization\n",
        "# Using Open-Source Bayesian-Optimization library:\n",
        "# https://github.com/fmfn/BayesianOptimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=585dbef8de1c0134bcb2a6aba72df78ae647a7b2ce12beba26ebd5a8755dd419\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvFzgZQr3AYX",
        "colab_type": "code",
        "outputId": "559de872-6f2c-4e82-8eee-67873ab5fffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "def func(lr_0, B, delta, p_1, p_2):\n",
        "    B = max(int(B*320),1)\n",
        "    cnn = CNN_Model(lr_0, delta, B, p_1, p_2)\n",
        "    cnn.define_model()\n",
        "    cnn.train_model(trX, trY, valX, valY)\n",
        "    val_loss = np.nanmin(cnn.train_history.history['val_loss'])\n",
        "    #print('loss:', val_loss)\n",
        "    del cnn\n",
        "    K.clear_session()\n",
        "    return (-1*val_loss) # nigative value to be fed to the maximize function\n",
        "\n",
        "#param_bounds = {'lr_0': (1e-6, 1e-2), 'delta': (1e-2, 1e-10), 'B': (32, 1024), 'p_1': (0.1, 0.5), 'p_2': (0.1, 0.5)}\n",
        "param_bounds = {\n",
        "    'lr_0': (1e-6, 1e-1),\n",
        "    'delta': (1e-10, 0),\n",
        "    'B': (0.1, 3.2),\n",
        "    'p_1': (0.1, 0.6),\n",
        "    'p_2': (0.1, 0.6)\n",
        "    }\n",
        "\n",
        "optimizer = BayesianOptimization(f=func, pbounds=param_bounds, verbose=2, random_state=42)\n",
        "optimizer.maximize(init_points=30, n_iter=70)\n",
        "print(optimizer.max)\n",
        "\n",
        "for i, res in enumerate(optimizer.res):\n",
        "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   |     B     |   delta   |   lr_0    |    p_1    |    p_2    |\n",
            "-------------------------------------------------------------------------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.203   \u001b[0m | \u001b[0m 1.261   \u001b[0m | \u001b[0m 4.929e-1\u001b[0m | \u001b[0m 0.0732  \u001b[0m | \u001b[0m 0.3993  \u001b[0m | \u001b[0m 0.178   \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.5836  \u001b[0m | \u001b[0m 9.419e-1\u001b[0m | \u001b[0m 0.08662 \u001b[0m | \u001b[0m 0.4006  \u001b[0m | \u001b[0m 0.454   \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.1638  \u001b[0m | \u001b[0m 3.009e-1\u001b[0m | \u001b[0m 0.08324 \u001b[0m | \u001b[0m 0.2062  \u001b[0m | \u001b[0m 0.1909  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-2.233   \u001b[0m | \u001b[0m 0.6686  \u001b[0m | \u001b[0m 6.958e-1\u001b[0m | \u001b[0m 0.05248 \u001b[0m | \u001b[0m 0.316   \u001b[0m | \u001b[0m 0.2456  \u001b[0m |\n",
            "| \u001b[95m 5       \u001b[0m | \u001b[95m-1.414   \u001b[0m | \u001b[95m 1.997   \u001b[0m | \u001b[95m 8.605e-1\u001b[0m | \u001b[95m 0.02922 \u001b[0m | \u001b[95m 0.2832  \u001b[0m | \u001b[95m 0.328   \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-1.995   \u001b[0m | \u001b[0m 2.534   \u001b[0m | \u001b[0m 8.003e-1\u001b[0m | \u001b[0m 0.05142 \u001b[0m | \u001b[0m 0.3962  \u001b[0m | \u001b[0m 0.1232  \u001b[0m |\n",
            "WARNING:tensorflow:Large dropout rate: 0.574443 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.582816 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.278   \u001b[0m | \u001b[0m 1.983   \u001b[0m | \u001b[0m 8.295e-1\u001b[0m | \u001b[0m 0.006506\u001b[0m | \u001b[0m 0.5744  \u001b[0m | \u001b[0m 0.5828  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.296   \u001b[0m | \u001b[0m 2.606   \u001b[0m | \u001b[0m 6.954e-1\u001b[0m | \u001b[0m 0.009768\u001b[0m | \u001b[0m 0.4421  \u001b[0m | \u001b[0m 0.3201  \u001b[0m |\n",
            "WARNING:tensorflow:Large dropout rate: 0.55466 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-2.18    \u001b[0m | \u001b[0m 0.4783  \u001b[0m | \u001b[0m 5.048e-1\u001b[0m | \u001b[0m 0.00344 \u001b[0m | \u001b[0m 0.5547  \u001b[0m | \u001b[0m 0.2294  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-1.83    \u001b[0m | \u001b[0m 2.154   \u001b[0m | \u001b[0m 6.883e-1\u001b[0m | \u001b[0m 0.05201 \u001b[0m | \u001b[0m 0.3734  \u001b[0m | \u001b[0m 0.1924  \u001b[0m |\n",
            "WARNING:tensorflow:Large dropout rate: 0.547414 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-2.279   \u001b[0m | \u001b[0m 3.106   \u001b[0m | \u001b[0m 2.249e-1\u001b[0m | \u001b[0m 0.09395 \u001b[0m | \u001b[0m 0.5474  \u001b[0m | \u001b[0m 0.3989  \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.066   \u001b[0m | \u001b[0m 2.958   \u001b[0m | \u001b[0m 9.115e-1\u001b[0m | \u001b[0m 0.0196  \u001b[0m | \u001b[0m 0.1226  \u001b[0m | \u001b[0m 0.2627  \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.179   \u001b[0m | \u001b[0m 1.305   \u001b[0m | \u001b[0m 7.287e-1\u001b[0m | \u001b[0m 0.08287 \u001b[0m | \u001b[0m 0.2784  \u001b[0m | \u001b[0m 0.2405  \u001b[0m |\n",
            "WARNING:tensorflow:Large dropout rate: 0.593443 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.241   \u001b[0m | \u001b[0m 1.782   \u001b[0m | \u001b[0m 8.591e-1\u001b[0m | \u001b[0m 0.08022 \u001b[0m | \u001b[0m 0.1373  \u001b[0m | \u001b[0m 0.5934  \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.299   \u001b[0m | \u001b[0m 2.494   \u001b[0m | \u001b[0m 8.013e-1\u001b[0m | \u001b[0m 0.000553\u001b[0m | \u001b[0m 0.5077  \u001b[0m | \u001b[0m 0.4534  \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.251   \u001b[0m | \u001b[0m 2.36    \u001b[0m | \u001b[0m 2.287e-1\u001b[0m | \u001b[0m 0.007405\u001b[0m | \u001b[0m 0.2792  \u001b[0m | \u001b[0m 0.1579  \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-1.624   \u001b[0m | \u001b[0m 2.776   \u001b[0m | \u001b[0m 3.767e-1\u001b[0m | \u001b[0m 0.03309 \u001b[0m | \u001b[0m 0.1318  \u001b[0m | \u001b[0m 0.2555  \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-2.051   \u001b[0m | \u001b[0m 1.108   \u001b[0m | \u001b[0m 2.704e-1\u001b[0m | \u001b[0m 0.06376 \u001b[0m | \u001b[0m 0.5436  \u001b[0m | \u001b[0m 0.3361  \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.296   \u001b[0m | \u001b[0m 0.4707  \u001b[0m | \u001b[0m 2.868e-1\u001b[0m | \u001b[0m 0.07608 \u001b[0m | \u001b[0m 0.3806  \u001b[0m | \u001b[0m 0.4855  \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m-2.098   \u001b[0m | \u001b[0m 1.631   \u001b[0m | \u001b[0m 4.773e-1\u001b[0m | \u001b[0m 0.04275 \u001b[0m | \u001b[0m 0.1127  \u001b[0m | \u001b[0m 0.1539  \u001b[0m |\n",
            "| \u001b[95m 21      \u001b[0m | \u001b[95m-1.134   \u001b[0m | \u001b[95m 0.1974  \u001b[0m | \u001b[95m 3.636e-1\u001b[0m | \u001b[95m 0.03144 \u001b[0m | \u001b[95m 0.3543  \u001b[0m | \u001b[95m 0.5538  \u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.289   \u001b[0m | \u001b[0m 0.8728  \u001b[0m | \u001b[0m 5.896e-1\u001b[0m | \u001b[0m 0.07556 \u001b[0m | \u001b[0m 0.2144  \u001b[0m | \u001b[0m 0.1385  \u001b[0m |\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.9982  \u001b[0m | \u001b[0m 8.388e-1\u001b[0m | \u001b[0m 0.09297 \u001b[0m | \u001b[0m 0.5041  \u001b[0m | \u001b[0m 0.4167  \u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.192   \u001b[0m | \u001b[0m 2.802   \u001b[0m | \u001b[0m 1.963e-1\u001b[0m | \u001b[0m 0.01866 \u001b[0m | \u001b[0m 0.5463  \u001b[0m | \u001b[0m 0.3697  \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.229   \u001b[0m | \u001b[0m 2.603   \u001b[0m | \u001b[0m 1.039e-1\u001b[0m | \u001b[0m 0.0318  \u001b[0m | \u001b[0m 0.155   \u001b[0m | \u001b[0m 0.214   \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-2.228   \u001b[0m | \u001b[0m 1.424   \u001b[0m | \u001b[0m 1.82e-11\u001b[0m | \u001b[0m 0.08607 \u001b[0m | \u001b[0m 0.1035  \u001b[0m | \u001b[0m 0.3554  \u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-1.901   \u001b[0m | \u001b[0m 1.394   \u001b[0m | \u001b[0m 7.779e-1\u001b[0m | \u001b[0m 0.01199 \u001b[0m | \u001b[0m 0.2688  \u001b[0m | \u001b[0m 0.5715  \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-2.271   \u001b[0m | \u001b[0m 1.102   \u001b[0m | \u001b[0m 4.812e-1\u001b[0m | \u001b[0m 0.0703  \u001b[0m | \u001b[0m 0.2818  \u001b[0m | \u001b[0m 0.5859  \u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-1.944   \u001b[0m | \u001b[0m 3.084   \u001b[0m | \u001b[0m 7.482e-1\u001b[0m | \u001b[0m 0.04973 \u001b[0m | \u001b[0m 0.2504  \u001b[0m | \u001b[0m 0.2424  \u001b[0m |\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.2143  \u001b[0m | \u001b[0m 3.904e-1\u001b[0m | \u001b[0m 0.05027 \u001b[0m | \u001b[0m 0.1257  \u001b[0m | \u001b[0m 0.2393  \u001b[0m |\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.1055  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0662  \u001b[0m | \u001b[0m 0.5968  \u001b[0m | \u001b[0m 0.599   \u001b[0m |\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m-1.17    \u001b[0m | \u001b[0m 0.103   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.03015 \u001b[0m | \u001b[0m 0.1349  \u001b[0m | \u001b[0m 0.5979  \u001b[0m |\n",
            "| \u001b[0m 33      \u001b[0m | \u001b[0m-1.884   \u001b[0m | \u001b[0m 1.774   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01826 \u001b[0m | \u001b[0m 0.5937  \u001b[0m | \u001b[0m 0.1182  \u001b[0m |\n",
            "| \u001b[0m 34      \u001b[0m | \u001b[0m-2.106   \u001b[0m | \u001b[0m 2.742   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.08981 \u001b[0m | \u001b[0m 0.1074  \u001b[0m | \u001b[0m 0.5961  \u001b[0m |\n",
            "| \u001b[0m 35      \u001b[0m | \u001b[0m-2.175   \u001b[0m | \u001b[0m 3.175   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.09633 \u001b[0m | \u001b[0m 0.5725  \u001b[0m | \u001b[0m 0.1042  \u001b[0m |\n",
            "| \u001b[0m 36      \u001b[0m | \u001b[0m-1.817   \u001b[0m | \u001b[0m 1.5     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.09534 \u001b[0m | \u001b[0m 0.5995  \u001b[0m | \u001b[0m 0.5606  \u001b[0m |\n",
            "| \u001b[0m 37      \u001b[0m | \u001b[0m-2.252   \u001b[0m | \u001b[0m 2.023   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.09673 \u001b[0m | \u001b[0m 0.1121  \u001b[0m | \u001b[0m 0.1024  \u001b[0m |\n",
            "| \u001b[0m 38      \u001b[0m | \u001b[0m-2.274   \u001b[0m | \u001b[0m 0.1023  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.000254\u001b[0m | \u001b[0m 0.3014  \u001b[0m | \u001b[0m 0.4797  \u001b[0m |\n",
            "| \u001b[0m 39      \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.3373  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.07967 \u001b[0m | \u001b[0m 0.1003  \u001b[0m | \u001b[0m 0.5969  \u001b[0m |\n",
            "| \u001b[0m 40      \u001b[0m | \u001b[0m-2.285   \u001b[0m | \u001b[0m 2.218   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.09928 \u001b[0m | \u001b[0m 0.1158  \u001b[0m | \u001b[0m 0.5994  \u001b[0m |\n",
            "| \u001b[0m 41      \u001b[0m | \u001b[0m-2.1     \u001b[0m | \u001b[0m 3.195   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.07851 \u001b[0m | \u001b[0m 0.142   \u001b[0m | \u001b[0m 0.5988  \u001b[0m |\n",
            "| \u001b[0m 42      \u001b[0m | \u001b[0m-2.125   \u001b[0m | \u001b[0m 0.8831  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04197 \u001b[0m | \u001b[0m 0.5941  \u001b[0m | \u001b[0m 0.1023  \u001b[0m |\n",
            "| \u001b[0m 43      \u001b[0m | \u001b[0m-2.271   \u001b[0m | \u001b[0m 1.724   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.005366\u001b[0m | \u001b[0m 0.4102  \u001b[0m | \u001b[0m 0.3489  \u001b[0m |\n",
            "| \u001b[0m 44      \u001b[0m | \u001b[0m-1.618   \u001b[0m | \u001b[0m 2.266   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0369  \u001b[0m | \u001b[0m 0.5957  \u001b[0m | \u001b[0m 0.1     \u001b[0m |\n",
            "| \u001b[0m 45      \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.1219  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.09016 \u001b[0m | \u001b[0m 0.2811  \u001b[0m | \u001b[0m 0.5977  \u001b[0m |\n",
            "| \u001b[0m 46      \u001b[0m | \u001b[0m-2.188   \u001b[0m | \u001b[0m 0.3244  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.002121\u001b[0m | \u001b[0m 0.4567  \u001b[0m | \u001b[0m 0.5963  \u001b[0m |\n",
            "| \u001b[0m 47      \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.129   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.07721 \u001b[0m | \u001b[0m 0.5911  \u001b[0m | \u001b[0m 0.1031  \u001b[0m |\n",
            "| \u001b[0m 48      \u001b[0m | \u001b[0m-2.242   \u001b[0m | \u001b[0m 2.05    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0955  \u001b[0m | \u001b[0m 0.598   \u001b[0m | \u001b[0m 0.2701  \u001b[0m |\n",
            "| \u001b[0m 49      \u001b[0m | \u001b[0m-2.061   \u001b[0m | \u001b[0m 2.853   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.09579 \u001b[0m | \u001b[0m 0.3405  \u001b[0m | \u001b[0m 0.1041  \u001b[0m |\n",
            "| \u001b[0m 50      \u001b[0m | \u001b[0m-2.111   \u001b[0m | \u001b[0m 1.5     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.08367 \u001b[0m | \u001b[0m 0.5987  \u001b[0m | \u001b[0m 0.11    \u001b[0m |\n",
            "| \u001b[0m 51      \u001b[0m | \u001b[0m-2.145   \u001b[0m | \u001b[0m 1.252   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01116 \u001b[0m | \u001b[0m 0.5962  \u001b[0m | \u001b[0m 0.5822  \u001b[0m |\n",
            "| \u001b[0m 52      \u001b[0m | \u001b[0m-1.885   \u001b[0m | \u001b[0m 0.7963  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.006172\u001b[0m | \u001b[0m 0.1001  \u001b[0m | \u001b[0m 0.5816  \u001b[0m |\n",
            "| \u001b[0m 53      \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 3.197   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.09787 \u001b[0m | \u001b[0m 0.1003  \u001b[0m | \u001b[0m 0.107   \u001b[0m |\n",
            "| \u001b[0m 54      \u001b[0m | \u001b[0m-2.078   \u001b[0m | \u001b[0m 2.929   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.02224 \u001b[0m | \u001b[0m 0.359   \u001b[0m | \u001b[0m 0.5977  \u001b[0m |\n",
            "| \u001b[0m 55      \u001b[0m | \u001b[0m-1.971   \u001b[0m | \u001b[0m 0.5319  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04262 \u001b[0m | \u001b[0m 0.1018  \u001b[0m | \u001b[0m 0.1078  \u001b[0m |\n",
            "| \u001b[0m 56      \u001b[0m | \u001b[0m-2.037   \u001b[0m | \u001b[0m 2.615   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.08149 \u001b[0m | \u001b[0m 0.5999  \u001b[0m | \u001b[0m 0.1058  \u001b[0m |\n",
            "| \u001b[0m 57      \u001b[0m | \u001b[0m-2.294   \u001b[0m | \u001b[0m 1.939   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.001338\u001b[0m | \u001b[0m 0.3786  \u001b[0m | \u001b[0m 0.111   \u001b[0m |\n",
            "| \u001b[0m 58      \u001b[0m | \u001b[0m-2.297   \u001b[0m | \u001b[0m 1.992   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.000534\u001b[0m | \u001b[0m 0.1001  \u001b[0m | \u001b[0m 0.3941  \u001b[0m |\n",
            "| \u001b[0m 59      \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.1856  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04843 \u001b[0m | \u001b[0m 0.3398  \u001b[0m | \u001b[0m 0.5465  \u001b[0m |\n",
            "| \u001b[0m 60      \u001b[0m | \u001b[0m-2.188   \u001b[0m | \u001b[0m 0.8903  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.06231 \u001b[0m | \u001b[0m 0.2518  \u001b[0m | \u001b[0m 0.2038  \u001b[0m |\n",
            "| \u001b[0m 61      \u001b[0m | \u001b[0m-1.483   \u001b[0m | \u001b[0m 2.415   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.03296 \u001b[0m | \u001b[0m 0.2786  \u001b[0m | \u001b[0m 0.5742  \u001b[0m |\n",
            "| \u001b[0m 62      \u001b[0m | \u001b[0m-2.087   \u001b[0m | \u001b[0m 1.401   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.083   \u001b[0m | \u001b[0m 0.3315  \u001b[0m | \u001b[0m 0.4803  \u001b[0m |\n",
            "| \u001b[0m 63      \u001b[0m | \u001b[0m-1.929   \u001b[0m | \u001b[0m 2.43    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0406  \u001b[0m | \u001b[0m 0.2743  \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
            "| \u001b[0m 64      \u001b[0m | \u001b[0m-1.991   \u001b[0m | \u001b[0m 1.807   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.07309 \u001b[0m | \u001b[0m 0.3559  \u001b[0m | \u001b[0m 0.4216  \u001b[0m |\n",
            "| \u001b[0m 65      \u001b[0m | \u001b[0m-2.245   \u001b[0m | \u001b[0m 3.05    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.009271\u001b[0m | \u001b[0m 0.1233  \u001b[0m | \u001b[0m 0.2651  \u001b[0m |\n",
            "| \u001b[0m 66      \u001b[0m | \u001b[0m-1.8     \u001b[0m | \u001b[0m 1.308   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01183 \u001b[0m | \u001b[0m 0.3148  \u001b[0m | \u001b[0m 0.4823  \u001b[0m |\n",
            "| \u001b[0m 67      \u001b[0m | \u001b[0m-2.279   \u001b[0m | \u001b[0m 2.955   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.008186\u001b[0m | \u001b[0m 0.3762  \u001b[0m | \u001b[0m 0.1357  \u001b[0m |\n",
            "| \u001b[0m 68      \u001b[0m | \u001b[0m-2.06    \u001b[0m | \u001b[0m 0.7025  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0688  \u001b[0m | \u001b[0m 0.313   \u001b[0m | \u001b[0m 0.1792  \u001b[0m |\n",
            "| \u001b[0m 69      \u001b[0m | \u001b[0m-2.249   \u001b[0m | \u001b[0m 0.5036  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.001826\u001b[0m | \u001b[0m 0.3584  \u001b[0m | \u001b[0m 0.447   \u001b[0m |\n",
            "| \u001b[0m 70      \u001b[0m | \u001b[0m-1.772   \u001b[0m | \u001b[0m 1.704   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.03665 \u001b[0m | \u001b[0m 0.3786  \u001b[0m | \u001b[0m 0.1091  \u001b[0m |\n",
            "| \u001b[0m 71      \u001b[0m | \u001b[0m-1.943   \u001b[0m | \u001b[0m 0.4581  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04488 \u001b[0m | \u001b[0m 0.3302  \u001b[0m | \u001b[0m 0.3493  \u001b[0m |\n",
            "| \u001b[0m 72      \u001b[0m | \u001b[0m-2.112   \u001b[0m | \u001b[0m 2.513   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.08118 \u001b[0m | \u001b[0m 0.4235  \u001b[0m | \u001b[0m 0.4095  \u001b[0m |\n",
            "| \u001b[0m 73      \u001b[0m | \u001b[0m-1.424   \u001b[0m | \u001b[0m 2.272   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.03085 \u001b[0m | \u001b[0m 0.4012  \u001b[0m | \u001b[0m 0.241   \u001b[0m |\n",
            "| \u001b[0m 74      \u001b[0m | \u001b[0m-2.046   \u001b[0m | \u001b[0m 2.087   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.06069 \u001b[0m | \u001b[0m 0.4496  \u001b[0m | \u001b[0m 0.5458  \u001b[0m |\n",
            "| \u001b[0m 75      \u001b[0m | \u001b[0m-2.073   \u001b[0m | \u001b[0m 3.106   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04935 \u001b[0m | \u001b[0m 0.3732  \u001b[0m | \u001b[0m 0.1898  \u001b[0m |\n",
            "| \u001b[0m 76      \u001b[0m | \u001b[0m-2.291   \u001b[0m | \u001b[0m 2.363   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.002083\u001b[0m | \u001b[0m 0.5061  \u001b[0m | \u001b[0m 0.3605  \u001b[0m |\n",
            "| \u001b[0m 77      \u001b[0m | \u001b[0m-2.109   \u001b[0m | \u001b[0m 0.987   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.05271 \u001b[0m | \u001b[0m 0.2246  \u001b[0m | \u001b[0m 0.1522  \u001b[0m |\n",
            "| \u001b[0m 78      \u001b[0m | \u001b[0m-2.161   \u001b[0m | \u001b[0m 2.967   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.09809 \u001b[0m | \u001b[0m 0.133   \u001b[0m | \u001b[0m 0.5769  \u001b[0m |\n",
            "| \u001b[0m 79      \u001b[0m | \u001b[0m-1.454   \u001b[0m | \u001b[0m 2.179   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.03283 \u001b[0m | \u001b[0m 0.4386  \u001b[0m | \u001b[0m 0.5454  \u001b[0m |\n",
            "| \u001b[0m 80      \u001b[0m | \u001b[0m-2.246   \u001b[0m | \u001b[0m 1.71    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.092   \u001b[0m | \u001b[0m 0.2824  \u001b[0m | \u001b[0m 0.5835  \u001b[0m |\n",
            "| \u001b[0m 81      \u001b[0m | \u001b[0m-2.303   \u001b[0m | \u001b[0m 0.1482  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.063   \u001b[0m | \u001b[0m 0.1746  \u001b[0m | \u001b[0m 0.2653  \u001b[0m |\n",
            "| \u001b[0m 82      \u001b[0m | \u001b[0m-2.107   \u001b[0m | \u001b[0m 1.617   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.07854 \u001b[0m | \u001b[0m 0.4761  \u001b[0m | \u001b[0m 0.4435  \u001b[0m |\n",
            "| \u001b[0m 83      \u001b[0m | \u001b[0m-1.763   \u001b[0m | \u001b[0m 0.9591  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.03599 \u001b[0m | \u001b[0m 0.5009  \u001b[0m | \u001b[0m 0.1479  \u001b[0m |\n",
            "| \u001b[0m 84      \u001b[0m | \u001b[0m-2.295   \u001b[0m | \u001b[0m 2.272   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.001441\u001b[0m | \u001b[0m 0.4485  \u001b[0m | \u001b[0m 0.5969  \u001b[0m |\n",
            "| \u001b[0m 85      \u001b[0m | \u001b[0m-2.12    \u001b[0m | \u001b[0m 0.6223  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.03139 \u001b[0m | \u001b[0m 0.3105  \u001b[0m | \u001b[0m 0.1519  \u001b[0m |\n",
            "| \u001b[0m 86      \u001b[0m | \u001b[0m-2.227   \u001b[0m | \u001b[0m 2.596   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.09    \u001b[0m | \u001b[0m 0.3127  \u001b[0m | \u001b[0m 0.54    \u001b[0m |\n",
            "| \u001b[0m 87      \u001b[0m | \u001b[0m-1.867   \u001b[0m | \u001b[0m 3.074   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.05502 \u001b[0m | \u001b[0m 0.5141  \u001b[0m | \u001b[0m 0.3697  \u001b[0m |\n",
            "| \u001b[0m 88      \u001b[0m | \u001b[0m-1.894   \u001b[0m | \u001b[0m 2.289   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.06645 \u001b[0m | \u001b[0m 0.5267  \u001b[0m | \u001b[0m 0.5204  \u001b[0m |\n",
            "| \u001b[0m 89      \u001b[0m | \u001b[0m-2.273   \u001b[0m | \u001b[0m 0.3132  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.05791 \u001b[0m | \u001b[0m 0.3822  \u001b[0m | \u001b[0m 0.435   \u001b[0m |\n",
            "| \u001b[0m 90      \u001b[0m | \u001b[0m-2.298   \u001b[0m | \u001b[0m 2.848   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.000915\u001b[0m | \u001b[0m 0.4601  \u001b[0m | \u001b[0m 0.508   \u001b[0m |\n",
            "| \u001b[0m 91      \u001b[0m | \u001b[0m-2.199   \u001b[0m | \u001b[0m 0.8674  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04518 \u001b[0m | \u001b[0m 0.4192  \u001b[0m | \u001b[0m 0.5955  \u001b[0m |\n",
            "| \u001b[0m 92      \u001b[0m | \u001b[0m-2.088   \u001b[0m | \u001b[0m 2.412   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.07696 \u001b[0m | \u001b[0m 0.2906  \u001b[0m | \u001b[0m 0.5841  \u001b[0m |\n",
            "| \u001b[0m 93      \u001b[0m | \u001b[0m-2.178   \u001b[0m | \u001b[0m 1.473   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.05484 \u001b[0m | \u001b[0m 0.4248  \u001b[0m | \u001b[0m 0.1709  \u001b[0m |\n",
            "| \u001b[0m 94      \u001b[0m | \u001b[0m-2.115   \u001b[0m | \u001b[0m 0.3283  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.05004 \u001b[0m | \u001b[0m 0.5837  \u001b[0m | \u001b[0m 0.353   \u001b[0m |\n",
            "| \u001b[0m 95      \u001b[0m | \u001b[0m-1.83    \u001b[0m | \u001b[0m 2.44    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04519 \u001b[0m | \u001b[0m 0.1173  \u001b[0m | \u001b[0m 0.5432  \u001b[0m |\n",
            "| \u001b[0m 96      \u001b[0m | \u001b[0m-2.32    \u001b[0m | \u001b[0m 1.736   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.08402 \u001b[0m | \u001b[0m 0.1624  \u001b[0m | \u001b[0m 0.165   \u001b[0m |\n",
            "| \u001b[0m 97      \u001b[0m | \u001b[0m-1.837   \u001b[0m | \u001b[0m 2.561   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04836 \u001b[0m | \u001b[0m 0.3038  \u001b[0m | \u001b[0m 0.5867  \u001b[0m |\n",
            "| \u001b[0m 98      \u001b[0m | \u001b[0m-1.435   \u001b[0m | \u001b[0m 2.012   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.02763 \u001b[0m | \u001b[0m 0.335   \u001b[0m | \u001b[0m 0.1262  \u001b[0m |\n",
            "| \u001b[0m 99      \u001b[0m | \u001b[0m-1.301   \u001b[0m | \u001b[0m 1.43    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.02072 \u001b[0m | \u001b[0m 0.1692  \u001b[0m | \u001b[0m 0.1778  \u001b[0m |\n",
            "| \u001b[0m 100     \u001b[0m | \u001b[0m-1.802   \u001b[0m | \u001b[0m 2.006   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.04833 \u001b[0m | \u001b[0m 0.1101  \u001b[0m | \u001b[0m 0.3681  \u001b[0m |\n",
            "=====================================================================================\n",
            "Iteration 0: \n",
            "\t{'target': -2.203275761604309, 'params': {'B': 1.2610743684268237, 'delta': 4.928569359008377e-12, 'lr_0': 0.0731996621871987, 'p_1': 0.3993292420985183, 'p_2': 0.17800932022121826}}\n",
            "Iteration 1: \n",
            "\t{'target': -2.3025858602523805, 'params': {'B': 0.5835830130422283, 'delta': 9.419163878318006e-11, 'lr_0': 0.08661774840134774, 'p_1': 0.40055750587160444, 'p_2': 0.4540362888980227}}\n",
            "Iteration 2: \n",
            "\t{'target': -2.302598253250122, 'params': {'B': 0.1638119323169876, 'delta': 3.0090147838005703e-12, 'lr_0': 0.08324443163740138, 'p_1': 0.20616955533913808, 'p_2': 0.19091248360355031}}\n",
            "Iteration 3: \n",
            "\t{'target': -2.233433811664581, 'params': {'B': 0.6685539805456449, 'delta': 6.957577570404623e-11, 'lr_0': 0.05247611840679216, 'p_1': 0.3159725093210579, 'p_2': 0.24561457009902096}}\n",
            "Iteration 4: \n",
            "\t{'target': -1.4143812656402588, 'params': {'B': 1.9967439736393764, 'delta': 8.605061393479582e-11, 'lr_0': 0.029215172708873284, 'p_1': 0.28318092164684583, 'p_2': 0.32803499210851794}}\n",
            "Iteration 5: \n",
            "\t{'target': -1.9948382377624512, 'params': {'B': 2.5340454803183423, 'delta': 8.003262178416403e-11, 'lr_0': 0.05142392960692275, 'p_1': 0.39620728443102127, 'p_2': 0.12322520635999887}}\n",
            "Iteration 6: \n",
            "\t{'target': -2.2779853343963623, 'params': {'B': 1.983389040894459, 'delta': 8.294758763127085e-11, 'lr_0': 0.006506094246934967, 'p_1': 0.5744427686266667, 'p_2': 0.5828160165372797}}\n",
            "Iteration 7: \n",
            "\t{'target': -2.295804023742676, 'params': {'B': 2.6060317791610297, 'delta': 6.953862308266294e-11, 'lr_0': 0.00976811372852438, 'p_1': 0.4421165132560785, 'p_2': 0.32007624686980063}}\n",
            "Iteration 8: \n",
            "\t{'target': -2.17972052526474, 'params': {'B': 0.47831852801881436, 'delta': 5.0482308988872984e-11, 'lr_0': 0.003439817723000725, 'p_1': 0.5546602010393911, 'p_2': 0.22938999080000846}}\n",
            "Iteration 9: \n",
            "\t{'target': -1.830053448677063, 'params': {'B': 2.1538190814973444, 'delta': 6.88288923910589e-11, 'lr_0': 0.052007282049759906, 'p_1': 0.3733551396716398, 'p_2': 0.19242722776276353}}\n",
            "Iteration 10: \n",
            "\t{'target': -2.2794382572174072, 'params': {'B': 3.1057123460701317, 'delta': 2.2486717663888545e-11, 'lr_0': 0.09394995465747735, 'p_1': 0.5474136752138244, 'p_2': 0.3989499894055426}}\n",
            "Iteration 11: \n",
            "\t{'target': -2.0661940574645996, 'params': {'B': 2.9578101285716625, 'delta': 9.115074979480805e-11, 'lr_0': 0.019599090259052102, 'p_1': 0.12261364445526904, 'p_2': 0.2626651653816322}}\n",
            "Iteration 12: \n",
            "\t{'target': -2.1791532974243166, 'params': {'B': 1.3048995980373943, 'delta': 7.286509682261042e-11, 'lr_0': 0.08287392217768379, 'p_1': 0.2783766633467947, 'p_2': 0.2404672548436904}}\n",
            "Iteration 13: \n",
            "\t{'target': -2.241156816482544, 'params': {'B': 1.7823578577905703, 'delta': 8.590757750252373e-11, 'lr_0': 0.08021989587842322, 'p_1': 0.13727532183988542, 'p_2': 0.5934434683002586}}\n",
            "Iteration 14: \n",
            "\t{'target': -2.2991068363189697, 'params': {'B': 2.4939587848196383, 'delta': 8.012843184658276e-11, 'lr_0': 0.0005532061902431164, 'p_1': 0.5077307142274171, 'p_2': 0.45342867192380854}}\n",
            "Iteration 15: \n",
            "\t{'target': -2.2511866092681885, 'params': {'B': 2.3599222209270607, 'delta': 2.287296533140542e-11, 'lr_0': 0.0074053911287573024, 'p_1': 0.27923286427213634, 'p_2': 0.15793452976256486}}\n",
            "Iteration 16: \n",
            "\t{'target': -1.6242343187332153, 'params': {'B': 2.77562062021434, 'delta': 3.767018731724421e-11, 'lr_0': 0.033090471587240065, 'p_1': 0.13177917514301182, 'p_2': 0.2554911608578311}}\n",
            "Iteration 17: \n",
            "\t{'target': -2.0510345983505247, 'params': {'B': 1.108068298282916, 'delta': 2.7039382166193597e-11, 'lr_0': 0.06375610957804996, 'p_1': 0.5436063712881632, 'p_2': 0.3361074625809747}}\n",
            "Iteration 18: \n",
            "\t{'target': -2.2964536905288697, 'params': {'B': 0.47074216240873534, 'delta': 2.8675521277700498e-11, 'lr_0': 0.07607874407664113, 'p_1': 0.38063859878474815, 'p_2': 0.48548358997728047}}\n",
            "Iteration 19: \n",
            "\t{'target': -2.098484754562378, 'params': {'B': 1.6307663487296113, 'delta': 4.7726717061800594e-11, 'lr_0': 0.042754674294836606, 'p_1': 0.1127095633720476, 'p_2': 0.15394571349665223}}\n",
            "Iteration 20: \n",
            "\t{'target': -1.1337814952135086, 'params': {'B': 0.19743047562887617, 'delta': 3.635895887362197e-11, 'lr_0': 0.03143628375165159, 'p_1': 0.35428534558235136, 'p_2': 0.5537832369630465}}\n",
            "Iteration 21: \n",
            "\t{'target': -2.289292606830597, 'params': {'B': 0.8728059103615123, 'delta': 5.896170769643703e-11, 'lr_0': 0.07555535830316633, 'p_1': 0.21439908274581124, 'p_2': 0.1384899549143965}}\n",
            "Iteration 22: \n",
            "\t{'target': -2.30258863735199, 'params': {'B': 0.9982295040326808, 'delta': 8.387787127459957e-11, 'lr_0': 0.09296983553660497, 'p_1': 0.5040601897822085, 'p_2': 0.41670187825521177}}\n",
            "Iteration 23: \n",
            "\t{'target': -2.1916162967681885, 'params': {'B': 2.801527829581925, 'delta': 1.9632792310088554e-11, 'lr_0': 0.0186578193185447, 'p_1': 0.5462794992449889, 'p_2': 0.3696711209578254}}\n",
            "Iteration 24: \n",
            "\t{'target': -2.229408025741577, 'params': {'B': 2.603064481008594, 'delta': 1.0390870007650672e-11, 'lr_0': 0.03180102949371142, 'p_1': 0.15502596226383838, 'p_2': 0.21396758127097085}}\n",
            "Iteration 25: \n",
            "\t{'target': -2.22793821811676, 'params': {'B': 1.4240341447413947, 'delta': 1.819852340775069e-11, 'lr_0': 0.0860731975950511, 'p_1': 0.10347606526559536, 'p_2': 0.35537365128878284}}\n",
            "Iteration 26: \n",
            "\t{'target': -1.9013852033615113, 'params': {'B': 1.393974109761215, 'delta': 7.778921895292698e-11, 'lr_0': 0.011987416868000946, 'p_1': 0.26880758570181396, 'p_2': 0.5714548519562596}}\n",
            "Iteration 27: \n",
            "\t{'target': -2.2706590538024902, 'params': {'B': 1.1019290892643414, 'delta': 4.812093782566339e-11, 'lr_0': 0.07030219287055889, 'p_1': 0.28181480118964697, 'p_2': 0.5858910413604803}}\n",
            "Iteration 28: \n",
            "\t{'target': -1.9437581300735474, 'params': {'B': 3.083586614320545, 'delta': 7.482177041746359e-11, 'lr_0': 0.04972535334073266, 'p_1': 0.25043915490838486, 'p_2': 0.2424202471887338}}\n",
            "Iteration 29: \n",
            "\t{'target': -2.302606737136841, 'params': {'B': 0.21434953679905167, 'delta': 3.904356660201032e-11, 'lr_0': 0.05026839964386293, 'p_1': 0.12573937562499468, 'p_2': 0.23932323211830572}}\n",
            "Iteration 30: \n",
            "\t{'target': -2.30269243144989, 'params': {'B': 0.10554368977749003, 'delta': 0.0, 'lr_0': 0.06619935138930022, 'p_1': 0.5967667916881095, 'p_2': 0.5989673613002808}}\n",
            "Iteration 31: \n",
            "\t{'target': -1.1702815523147583, 'params': {'B': 0.10298615760006018, 'delta': 0.0, 'lr_0': 0.030153964104483025, 'p_1': 0.1349465037224167, 'p_2': 0.5979140257047085}}\n",
            "Iteration 32: \n",
            "\t{'target': -1.8843600749969482, 'params': {'B': 1.7739410814531384, 'delta': 0.0, 'lr_0': 0.018260526072353674, 'p_1': 0.5936509914839063, 'p_2': 0.11820374861027508}}\n",
            "Iteration 33: \n",
            "\t{'target': -2.1058287620544434, 'params': {'B': 2.7415818023866203, 'delta': 0.0, 'lr_0': 0.08980964043948997, 'p_1': 0.10737932864177066, 'p_2': 0.5960852465636237}}\n",
            "Iteration 34: \n",
            "\t{'target': -2.1746084690093994, 'params': {'B': 3.174568932405227, 'delta': 0.0, 'lr_0': 0.09632974503575531, 'p_1': 0.5725483430035893, 'p_2': 0.10415326703877362}}\n",
            "Iteration 35: \n",
            "\t{'target': -1.81681986451149, 'params': {'B': 1.499842155910715, 'delta': 0.0, 'lr_0': 0.09533955392930571, 'p_1': 0.5994985768922665, 'p_2': 0.5606202289435186}}\n",
            "Iteration 36: \n",
            "\t{'target': -2.25240421295166, 'params': {'B': 2.0226572769311346, 'delta': 0.0, 'lr_0': 0.09673018921476623, 'p_1': 0.1121148522709274, 'p_2': 0.10235388208346005}}\n",
            "Iteration 37: \n",
            "\t{'target': -2.2742952289581297, 'params': {'B': 0.10225179394826274, 'delta': 0.0, 'lr_0': 0.00025438496512140366, 'p_1': 0.30141026912671853, 'p_2': 0.4796508106170454}}\n",
            "Iteration 38: \n",
            "\t{'target': -2.3025891289711, 'params': {'B': 0.3372601832274374, 'delta': 0.0, 'lr_0': 0.07967012432208091, 'p_1': 0.10033375275588366, 'p_2': 0.5968890696355912}}\n",
            "Iteration 39: \n",
            "\t{'target': -2.2852063179016113, 'params': {'B': 2.21766706696432, 'delta': 0.0, 'lr_0': 0.09927560824825951, 'p_1': 0.11579270393899257, 'p_2': 0.5994379752080778}}\n",
            "Iteration 40: \n",
            "\t{'target': -2.100222110748291, 'params': {'B': 3.195149269096013, 'delta': 0.0, 'lr_0': 0.07851121482578946, 'p_1': 0.14196451070906627, 'p_2': 0.5987661425239501}}\n",
            "Iteration 41: \n",
            "\t{'target': -2.12484663772583, 'params': {'B': 0.8831162816980715, 'delta': 0.0, 'lr_0': 0.041967974343709476, 'p_1': 0.5941112691917444, 'p_2': 0.10234107200453638}}\n",
            "Iteration 42: \n",
            "\t{'target': -2.2705256938934326, 'params': {'B': 1.7242618430572663, 'delta': 0.0, 'lr_0': 0.005366269956205014, 'p_1': 0.4101767693209688, 'p_2': 0.34885002281121624}}\n",
            "Iteration 43: \n",
            "\t{'target': -1.6176148653030396, 'params': {'B': 2.266023499720068, 'delta': 0.0, 'lr_0': 0.03690225050836964, 'p_1': 0.5957117957360458, 'p_2': 0.10002603377271554}}\n",
            "Iteration 44: \n",
            "\t{'target': -2.3026659021377562, 'params': {'B': 0.12188560402549767, 'delta': 0.0, 'lr_0': 0.09016376630465353, 'p_1': 0.2810773799658022, 'p_2': 0.5976621750573019}}\n",
            "Iteration 45: \n",
            "\t{'target': -2.18817195892334, 'params': {'B': 0.32440072958216415, 'delta': 0.0, 'lr_0': 0.002120665889779291, 'p_1': 0.45670629387786865, 'p_2': 0.5962772862576573}}\n",
            "Iteration 46: \n",
            "\t{'target': -2.3026410040855407, 'params': {'B': 0.12903517155926664, 'delta': 0.0, 'lr_0': 0.07720935818687365, 'p_1': 0.5911482490892472, 'p_2': 0.10313789703425771}}\n",
            "Iteration 47: \n",
            "\t{'target': -2.241534471511841, 'params': {'B': 2.049965888876177, 'delta': 0.0, 'lr_0': 0.09550254460989975, 'p_1': 0.5980045011976785, 'p_2': 0.27010839452260815}}\n",
            "Iteration 48: \n",
            "\t{'target': -2.0611610412597656, 'params': {'B': 2.8526275499467197, 'delta': 0.0, 'lr_0': 0.09579330610636519, 'p_1': 0.3405013866925163, 'p_2': 0.1041337983551652}}\n",
            "Iteration 49: \n",
            "\t{'target': -2.1112470245361328, 'params': {'B': 1.4995499251598519, 'delta': 0.0, 'lr_0': 0.08366977097824499, 'p_1': 0.5987196140146718, 'p_2': 0.10997652894936025}}\n",
            "Iteration 50: \n",
            "\t{'target': -2.1450853824615477, 'params': {'B': 1.2521125455026862, 'delta': 0.0, 'lr_0': 0.011163940378837738, 'p_1': 0.5961853875050623, 'p_2': 0.5821760228597329}}\n",
            "Iteration 51: \n",
            "\t{'target': -1.8848000917434693, 'params': {'B': 0.7963173787614072, 'delta': 0.0, 'lr_0': 0.0061718561067594345, 'p_1': 0.10009274233337442, 'p_2': 0.581587073512937}}\n",
            "Iteration 52: \n",
            "\t{'target': -2.302638053894043, 'params': {'B': 3.197003329022749, 'delta': 0.0, 'lr_0': 0.09787250591252544, 'p_1': 0.10027862328815959, 'p_2': 0.10699389084473812}}\n",
            "Iteration 53: \n",
            "\t{'target': -2.0778884887695312, 'params': {'B': 2.929077814674426, 'delta': 0.0, 'lr_0': 0.022240144492010935, 'p_1': 0.35900575578287053, 'p_2': 0.5976593687203692}}\n",
            "Iteration 54: \n",
            "\t{'target': -1.9707502961158752, 'params': {'B': 0.5319416392050936, 'delta': 0.0, 'lr_0': 0.042616519252022894, 'p_1': 0.10175818759819741, 'p_2': 0.10780350067232111}}\n",
            "Iteration 55: \n",
            "\t{'target': -2.037355661392212, 'params': {'B': 2.615198938802623, 'delta': 0.0, 'lr_0': 0.08149383027862643, 'p_1': 0.5998585248960245, 'p_2': 0.10580798163138608}}\n",
            "Iteration 56: \n",
            "\t{'target': -2.2941830158233643, 'params': {'B': 1.9386557990688444, 'delta': 0.0, 'lr_0': 0.0013377231250798877, 'p_1': 0.3786448206498684, 'p_2': 0.11104059630709337}}\n",
            "Iteration 57: \n",
            "\t{'target': -2.297316074371338, 'params': {'B': 1.9923104980803004, 'delta': 0.0, 'lr_0': 0.0005346717617116364, 'p_1': 0.10014336039316948, 'p_2': 0.39408119267253117}}\n",
            "Iteration 58: \n",
            "\t{'target': -2.302590913772583, 'params': {'B': 0.1855571450740034, 'delta': 0.0, 'lr_0': 0.04843493098745035, 'p_1': 0.3398312401009377, 'p_2': 0.5465410818577746}}\n",
            "Iteration 59: \n",
            "\t{'target': -2.188079605102539, 'params': {'B': 0.8903355318406521, 'delta': 0.0, 'lr_0': 0.06230963825633369, 'p_1': 0.2517710782963344, 'p_2': 0.20376453648185808}}\n",
            "Iteration 60: \n",
            "\t{'target': -1.4834372997283936, 'params': {'B': 2.4146108475991395, 'delta': 0.0, 'lr_0': 0.032962914455779056, 'p_1': 0.2786453717715882, 'p_2': 0.5741843989158423}}\n",
            "Iteration 61: \n",
            "\t{'target': -2.0867719383239747, 'params': {'B': 1.4007040194588054, 'delta': 0.0, 'lr_0': 0.08300004815431816, 'p_1': 0.33146402973308886, 'p_2': 0.4802665819250077}}\n",
            "Iteration 62: \n",
            "\t{'target': -1.92948579788208, 'params': {'B': 2.4304836295989634, 'delta': 0.0, 'lr_0': 0.04059537609091198, 'p_1': 0.2742560455615797, 'p_2': 0.2999912062264479}}\n",
            "Iteration 63: \n",
            "\t{'target': -1.991079330444336, 'params': {'B': 1.806852895051429, 'delta': 0.0, 'lr_0': 0.07309256048530884, 'p_1': 0.3558912137628528, 'p_2': 0.421634240189087}}\n",
            "Iteration 64: \n",
            "\t{'target': -2.2452869415283203, 'params': {'B': 3.049888517270779, 'delta': 0.0, 'lr_0': 0.009271436484030159, 'p_1': 0.12334939570718881, 'p_2': 0.26513033755582227}}\n",
            "Iteration 65: \n",
            "\t{'target': -1.7995770468711854, 'params': {'B': 1.3075990574999836, 'delta': 0.0, 'lr_0': 0.011832702553449122, 'p_1': 0.31479718289925307, 'p_2': 0.4822521217376915}}\n",
            "Iteration 66: \n",
            "\t{'target': -2.2789320945739746, 'params': {'B': 2.9552151906247084, 'delta': 0.0, 'lr_0': 0.008185939606361626, 'p_1': 0.37616179642257475, 'p_2': 0.13573121022787246}}\n",
            "Iteration 67: \n",
            "\t{'target': -2.0595375671386718, 'params': {'B': 0.7024719858551252, 'delta': 0.0, 'lr_0': 0.0688021137827687, 'p_1': 0.3130089041846038, 'p_2': 0.17918813633978273}}\n",
            "Iteration 68: \n",
            "\t{'target': -2.249162134170532, 'params': {'B': 0.503595500089095, 'delta': 0.0, 'lr_0': 0.0018258977062068592, 'p_1': 0.3583573712647694, 'p_2': 0.4470403996738046}}\n",
            "Iteration 69: \n",
            "\t{'target': -1.7717370986938477, 'params': {'B': 1.7039353813929754, 'delta': 0.0, 'lr_0': 0.03665194510504741, 'p_1': 0.3785805131245199, 'p_2': 0.1091132915260081}}\n",
            "Iteration 70: \n",
            "\t{'target': -1.9429967799186707, 'params': {'B': 0.458058647705365, 'delta': 0.0, 'lr_0': 0.04488222937107288, 'p_1': 0.33024852485162526, 'p_2': 0.3492561299681518}}\n",
            "Iteration 71: \n",
            "\t{'target': -2.1119728088378906, 'params': {'B': 2.5133596012936925, 'delta': 0.0, 'lr_0': 0.08117955241614412, 'p_1': 0.4235086952029693, 'p_2': 0.4095412627167164}}\n",
            "Iteration 72: \n",
            "\t{'target': -1.4239455461502075, 'params': {'B': 2.2722570170016443, 'delta': 0.0, 'lr_0': 0.03085236047244826, 'p_1': 0.40123863305704943, 'p_2': 0.24104471432003968}}\n",
            "Iteration 73: \n",
            "\t{'target': -2.0457565784454346, 'params': {'B': 2.087119667427462, 'delta': 0.0, 'lr_0': 0.060692067293805385, 'p_1': 0.4496378392469371, 'p_2': 0.5458222018358327}}\n",
            "Iteration 74: \n",
            "\t{'target': -2.0725135803222656, 'params': {'B': 3.1062985777638037, 'delta': 0.0, 'lr_0': 0.04935304434728445, 'p_1': 0.3731898614404068, 'p_2': 0.1897788064496347}}\n",
            "Iteration 75: \n",
            "\t{'target': -2.2905795574188232, 'params': {'B': 2.3634602108006573, 'delta': 0.0, 'lr_0': 0.0020829748222461948, 'p_1': 0.5061375647894752, 'p_2': 0.3604548416743095}}\n",
            "Iteration 76: \n",
            "\t{'target': -2.1091286897659303, 'params': {'B': 0.9869872648653869, 'delta': 0.0, 'lr_0': 0.05270760701849299, 'p_1': 0.22457799407846488, 'p_2': 0.1521529513040106}}\n",
            "Iteration 77: \n",
            "\t{'target': -2.161391496658325, 'params': {'B': 2.9670908907985867, 'delta': 0.0, 'lr_0': 0.09808616540675212, 'p_1': 0.13295804352174448, 'p_2': 0.5768782516345499}}\n",
            "Iteration 78: \n",
            "\t{'target': -1.4540584087371826, 'params': {'B': 2.179291111805235, 'delta': 0.0, 'lr_0': 0.03282882025098495, 'p_1': 0.4385573010020448, 'p_2': 0.5454303594238646}}\n",
            "Iteration 79: \n",
            "\t{'target': -2.2464585304260254, 'params': {'B': 1.7100324843886499, 'delta': 0.0, 'lr_0': 0.0919950721516412, 'p_1': 0.2824298556765986, 'p_2': 0.5835309571856113}}\n",
            "Iteration 80: \n",
            "\t{'target': -2.302595458507538, 'params': {'B': 0.1482139669549869, 'delta': 0.0, 'lr_0': 0.06300336880554516, 'p_1': 0.17464528855189268, 'p_2': 0.265339547556005}}\n",
            "Iteration 81: \n",
            "\t{'target': -2.1071298122406006, 'params': {'B': 1.6167230768508583, 'delta': 0.0, 'lr_0': 0.07854496074371235, 'p_1': 0.47610969843449236, 'p_2': 0.44349762098988654}}\n",
            "Iteration 82: \n",
            "\t{'target': -1.76346195602417, 'params': {'B': 0.959149879844672, 'delta': 0.0, 'lr_0': 0.03599101362205171, 'p_1': 0.500917649443382, 'p_2': 0.14788937010190265}}\n",
            "Iteration 83: \n",
            "\t{'target': -2.2950892448425293, 'params': {'B': 2.2719042704570374, 'delta': 0.0, 'lr_0': 0.0014412766787379981, 'p_1': 0.4485319119901392, 'p_2': 0.5969116563876968}}\n",
            "Iteration 84: \n",
            "\t{'target': -2.119574460029602, 'params': {'B': 0.6223137900238074, 'delta': 0.0, 'lr_0': 0.03138750238473765, 'p_1': 0.31046261956698773, 'p_2': 0.15193956333784484}}\n",
            "Iteration 85: \n",
            "\t{'target': -2.226900577545166, 'params': {'B': 2.5959162606004704, 'delta': 0.0, 'lr_0': 0.09000499755566822, 'p_1': 0.31269198102621354, 'p_2': 0.5400328747366112}}\n",
            "Iteration 86: \n",
            "\t{'target': -1.8669359683990479, 'params': {'B': 3.07372544837438, 'delta': 0.0, 'lr_0': 0.055021532467660374, 'p_1': 0.5140713875481188, 'p_2': 0.3697303013327158}}\n",
            "Iteration 87: \n",
            "\t{'target': -1.8941489458084106, 'params': {'B': 2.288512280805644, 'delta': 0.0, 'lr_0': 0.06644877617869817, 'p_1': 0.5266960071368776, 'p_2': 0.5203921636432505}}\n",
            "Iteration 88: \n",
            "\t{'target': -2.272987985610962, 'params': {'B': 0.3132266094020376, 'delta': 0.0, 'lr_0': 0.05791038424871959, 'p_1': 0.3821930237768121, 'p_2': 0.4350046684188169}}\n",
            "Iteration 89: \n",
            "\t{'target': -2.2978386878967285, 'params': {'B': 2.8479895819104084, 'delta': 0.0, 'lr_0': 0.0009155149852143695, 'p_1': 0.46010755587284946, 'p_2': 0.5080108182143541}}\n",
            "Iteration 90: \n",
            "\t{'target': -2.199273265838623, 'params': {'B': 0.8674408301777653, 'delta': 0.0, 'lr_0': 0.045178465738364496, 'p_1': 0.4191553795741141, 'p_2': 0.5954551506964944}}\n",
            "Iteration 91: \n",
            "\t{'target': -2.088059425354004, 'params': {'B': 2.4121031035088913, 'delta': 0.0, 'lr_0': 0.07696208886082866, 'p_1': 0.2906474958196811, 'p_2': 0.5840697119999313}}\n",
            "Iteration 92: \n",
            "\t{'target': -2.177536385536194, 'params': {'B': 1.4725798788281912, 'delta': 0.0, 'lr_0': 0.054838327370591106, 'p_1': 0.42476257472122847, 'p_2': 0.17085357346810512}}\n",
            "Iteration 93: \n",
            "\t{'target': -2.1151313519477846, 'params': {'B': 0.32829267287813113, 'delta': 0.0, 'lr_0': 0.050038829309489684, 'p_1': 0.5836682413184295, 'p_2': 0.35303513535154984}}\n",
            "Iteration 94: \n",
            "\t{'target': -1.829990267753601, 'params': {'B': 2.4396835820069507, 'delta': 0.0, 'lr_0': 0.045190088799814725, 'p_1': 0.1173165913788706, 'p_2': 0.5431851698743915}}\n",
            "Iteration 95: \n",
            "\t{'target': -2.319891929626465, 'params': {'B': 1.735812711685115, 'delta': 0.0, 'lr_0': 0.08402199183936317, 'p_1': 0.1623572503235783, 'p_2': 0.1650413873830968}}\n",
            "Iteration 96: \n",
            "\t{'target': -1.8370393514633179, 'params': {'B': 2.561006052221371, 'delta': 0.0, 'lr_0': 0.04836269346674877, 'p_1': 0.30375265899791126, 'p_2': 0.5867053945330141}}\n",
            "Iteration 97: \n",
            "\t{'target': -1.4354487657546997, 'params': {'B': 2.0124815161250496, 'delta': 0.0, 'lr_0': 0.02763312127880752, 'p_1': 0.3349999316027311, 'p_2': 0.1261932535570606}}\n",
            "Iteration 98: \n",
            "\t{'target': -1.301388100862503, 'params': {'B': 1.4296759215761887, 'delta': 0.0, 'lr_0': 0.02071748845816702, 'p_1': 0.16921397234638572, 'p_2': 0.17784464794455315}}\n",
            "Iteration 99: \n",
            "\t{'target': -1.8023591041564941, 'params': {'B': 2.0058868371034886, 'delta': 0.0, 'lr_0': 0.04832713005574424, 'p_1': 0.11012253176443518, 'p_2': 0.36813079508829705}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4HqRwgPE_Gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}